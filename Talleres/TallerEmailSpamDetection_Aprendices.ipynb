{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isandrade-udea/LabIA/blob/main/TallerEmailSpamDetection_Aprendices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9QzBf6m-FC1"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"140px\" src=\"https://raw.githubusercontent.com/isandrade-udea/LabIA/main/Captura%20desde%202024-05-24%2012-44-56.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "# **Email Spam Detection** üíå\n",
        "\n",
        "El taller muestra c√≥mo crear un modelo para clasificar spam en SMS. Utiliza el conjunto de datos SMS Spam Collection de la plataforma [Kaggle](https://www.kaggle.com/datasets/venky73/spam-mails-dataset/code?datasetId=109196&sortBy=voteCount) y esta baso en el notebook de [ZABIHULLAH18](https://www.kaggle.com/code/zabihullah18/email-spam-detection). Al final, se tendra una herramientapara filtrar mensajes no deseados y hacer m√°s segura la experiencia de mensajer√≠a de texto.\n",
        "\n",
        "**Instructores:**\n",
        "\n",
        "* Herna Villar\n",
        "\n",
        "* Isabel C. Andrade M."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb8XS2fA51S"
      },
      "source": [
        "#**1. Aim**\n",
        "\n",
        "\n",
        "<p><img alt=\"model\" height=\"240px\" src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*mbFBPcPUJD-53v3h.png\" align=\"centering\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "El objetivo es generar un modelo predictivo que clasifique los SMS en spam o ham."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQkrR_SlCoGo"
      },
      "source": [
        "#**2. Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tstQCrp-Jdn"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np        # For numerical operations\n",
        "import pandas as pd       # For data manipulation and analysis\n",
        "import matplotlib.pyplot as plt  # For data visualization\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set()\n",
        "plt.rcParams['figure.figsize'] = [7, 5]\n",
        "plt.rcParams['legend.fontsize'] = 16\n",
        "\n",
        "# Importing WordCloud for text visualization\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Importing NLTK for natural language processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords    # For stopwords\n",
        "\n",
        "\n",
        "# Downloading NLTK data\n",
        "nltk.download('stopwords')   # Downloading stopwords data\n",
        "nltk.download('punkt')       # Downloading tokenizer data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULubcH5kDNVb"
      },
      "source": [
        "#**3. Loading the data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  dataset\n",
        "path_to_file = \"https://raw.githubusercontent.com/isandrade-udea/datasets/main/spam.csv\" # @param {type:\"string\"}\n",
        "\n",
        "df = pd.read_csv(path_to_file,encoding='latin1')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QFKjy8ZTj6Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gugPOBb8GYEi"
      },
      "source": [
        "#**4. Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjNmjHtMGqJl"
      },
      "source": [
        "##Data Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CorCKOFFovm"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6wH_W_wGxIK"
      },
      "source": [
        "##Drop the Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBZ7ul9KHK2p"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlS3zYkHGhVs"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns = ['Unnamed: 2',\t'Unnamed: 3',\t'Unnamed: 4'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e9dHfauHRKQ"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Z26rl5IqzO"
      },
      "source": [
        "##Rename the Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPLsFHw8H0uN"
      },
      "outputs": [],
      "source": [
        "# Rename the columns name\n",
        "df.rename(columns = {'v1': 'target', 'v2':'text'}, inplace = True)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T034eu3I7cs"
      },
      "source": [
        "##Convert the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrv2eMYxIvgV"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "df['target'] = encoder.fit_transform(df['target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrlkFZuoJ12p"
      },
      "source": [
        "##Check Duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDYPG_71JHoT"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhDF7iIIJ801"
      },
      "source": [
        "##Remove Duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n80Gau_bJ5DF"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(keep = 'first')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjxEwrfEKJYh"
      },
      "source": [
        "##Shape of the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLTpyLkdKB-a"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoidU27tKRlA"
      },
      "source": [
        "#**5. EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGXxKRKKoc5"
      },
      "source": [
        "##Percentage of Ham and Spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_urzsILKN12"
      },
      "outputs": [],
      "source": [
        "values = df['target'].value_counts()\n",
        "total = values.sum()\n",
        "\n",
        "percentage_0 = (values[0] /total) * 100\n",
        "percentage_1 = (values[1]/ total) *100\n",
        "\n",
        "print('percentage of 0 :' ,percentage_0)\n",
        "print('percentage of 1 :' ,percentage_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP_8oJQDKtKw"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.pie(\n",
        "    values, labels=['ham', 'spam'],\n",
        "    autopct='%0.2f%%',\n",
        "    startangle=90,\n",
        "    wedgeprops={'linewidth': 2, 'edgecolor': 'white'},\n",
        "    shadow=True  # Add shadow\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWaR42UCMLFA"
      },
      "source": [
        "##Text Length and Structure Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29RBDflgN3e3"
      },
      "source": [
        "**Tokenize:**   es una herramienta de NLTK (Natural Language Toolkit)  para dividir el texto en palabras individuales o tokens\n",
        "\n",
        "Text:\n",
        "\n",
        " `df['text'][1] = Subject: hpl nom for january 9 , 2001\\r\\n( see attached file : hplnol 09 . xls )\\r\\n- hplnol 09 . xls`\n",
        "\n",
        "tokenize:\n",
        "\n",
        " `nltk.word_tokenize(df['text'][1]) = ['Subject', ':', 'hpl','nom','for', 'january',...`]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW-OkFSnLHlR"
      },
      "outputs": [],
      "source": [
        "df.loc[:, 'num_characters'] = df['text'].apply(len)\n",
        "df.loc[:,'num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
        "df.loc[:,'num_sentence'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zro-TdiGMyKA"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsjipt_8QStB"
      },
      "outputs": [],
      "source": [
        "df[['num_characters', 'num_words', 'num_sentence']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ3F5oQBSKOW"
      },
      "source": [
        "##Summary Statistics for Legitimate Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFsyG6GeQ9JQ"
      },
      "outputs": [],
      "source": [
        "df[df['target'] == 0][['num_characters', 'num_words', 'num_sentence']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jro_b4fSW69"
      },
      "source": [
        "##Summary Statistics for Spam Messages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"model\" height=\"70px\" src=\"https://raw.githubusercontent.com/isandrade-udea/LabIA/main/Captura%20desde%202024-05-24%2012-44-30.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "\n",
        "**Ejercicio**:\n",
        "Filtra el DataFrame df para seleccionar las filas donde 'target' sea igual a 1. Luego, muestra las estad√≠sticas descriptivas de las columnas 'num_characters', 'num_words' y 'num_sentence'.\n"
      ],
      "metadata": {
        "id": "7Nm5k4QDfJj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['target'] == 1][['num_characters', 'num_words', 'num_sentence']].describe()"
      ],
      "metadata": {
        "id": "V9jqp6QnU4j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgbi7_i2SxlO"
      },
      "source": [
        "##Character Length Distribution for Legitimate and Spam Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJTzpc_uSZnT"
      },
      "outputs": [],
      "source": [
        "# Create a figure and set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the histogram for target 0 in blue\n",
        "sns.histplot(df[df['target'] == 0]['num_characters'], color='blue', label='Target 0', kde=True)\n",
        "\n",
        "# Plot the histogram for target 1 in red\n",
        "sns.histplot(df[df['target'] == 1]['num_characters'], color='red', label='Target 1', kde=True)\n",
        "\n",
        "# Add labels and a title\n",
        "plt.xlabel('Number of Characters', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.title('Distribution of Number of Characters by Target', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Count Distribution for Legitimate and Spam Messages"
      ],
      "metadata": {
        "id": "9qIjN00HIgq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"model\" height=\"70px\" src=\"https://raw.githubusercontent.com/isandrade-udea/LabIA/main/Captura%20desde%202024-05-24%2012-44-30.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "**Ejercicio**:\n",
        "Muestra en una sola grafica la distribuci√≥n del recuento de palabras para mensajes ham y spam\n"
      ],
      "metadata": {
        "id": "FRKDYf-ZggvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure and set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the histogram for target 0 in blue\n",
        "sns.histplot(df[df['target'] == 0]['num_words'], color='blue', label='Target 0', kde=True)\n",
        "\n",
        "# Plot the histogram for target 1 in red\n",
        "sns.histplot(df[df['target'] == 1]['num_words'], color='red', label='Target 1', kde=True)\n",
        "\n",
        "# Add labels and a title\n",
        "plt.xlabel('Number of Characters', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.title('Distribution of Number of words by Target', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "MahPIurwWlfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Data Preprocessing**"
      ],
      "metadata": {
        "id": "i1yCNCjpG9ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"model\" height=\"340px\" src=\"https://raw.githubusercontent.com/isandrade-udea/datasets/7dce70e30f4e7af85331857e022b9c8d183c3d11/Captura%20desde%202024-05-23%2015-32-22.png\" align=\"centering\" hspace=\"10px\" vspace=\"0px\"></p>"
      ],
      "metadata": {
        "id": "yAawaFnMTID2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Porter Stemmer for text stemming\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Importing the string module for handling special characters\n",
        "import string\n",
        "\n",
        "# Creating an instance of the Porter Stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Lowercase transformation and text preprocessing function\n",
        "def transform_text(text):\n",
        "    # Transform the text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization using NLTK\n",
        "    text = nltk.word_tokenize(text)\n",
        "\n",
        "    # Removing special characters\n",
        "    y = []\n",
        "    for i in text:\n",
        "        if i.isalnum():\n",
        "            y.append(i)\n",
        "\n",
        "    # Removing stop words and punctuation\n",
        "    text = y[:]\n",
        "    y.clear()\n",
        "\n",
        "    # Loop through the tokens and remove stopwords and punctuation\n",
        "    for i in text:\n",
        "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
        "            y.append(i)\n",
        "\n",
        "    # Stemming using Porter Stemmer\n",
        "    text = y[:]\n",
        "    y.clear()\n",
        "    for i in text:\n",
        "        y.append(ps.stem(i))\n",
        "\n",
        "    # Join the processed tokens back into a single string\n",
        "    return \" \".join(y)"
      ],
      "metadata": {
        "id": "rUf3vUhlG4ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_text('This in $ an Example')"
      ],
      "metadata": {
        "id": "Ht4kmEspYABo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a New Column: 'transformed_text'"
      ],
      "metadata": {
        "id": "7vt0Na7CLEGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['transformed_text'] = df['text'].apply(transform_text)"
      ],
      "metadata": {
        "id": "llH5iv0hLFbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "6AZad-1ZLI2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Cloud for Spam Messages"
      ],
      "metadata": {
        "id": "X_EMuRHlLTjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wc = WordCloud(width = 500, height = 500, min_font_size = 10, background_color = 'white')\n",
        "spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep = \" \"))\n",
        "plt.figure(figsize = (15,6))\n",
        "plt.imshow(spam_wc)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jt8dh7MXLR0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Cloud for Not spam Messages"
      ],
      "metadata": {
        "id": "RULWQ6eWLajK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"model\" height=\"70px\" src=\"https://raw.githubusercontent.com/isandrade-udea/LabIA/main/Captura%20desde%202024-05-24%2012-44-30.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "**Ejercicio:**\n",
        "Muestra la nube de palabras para mensajes ham\n",
        "\n"
      ],
      "metadata": {
        "id": "dQjxLCX5g90y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Find top 30 words of spam"
      ],
      "metadata": {
        "id": "8tOwz1ySLtyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_carpos = []\n",
        "for sentence in df[df['target'] == 1]['transformed_text'].tolist():\n",
        "    for word in sentence.split():\n",
        "        spam_carpos.append(word)"
      ],
      "metadata": {
        "id": "MEJAxYx7LkCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "filter_df = pd.DataFrame(Counter(spam_carpos).most_common(30))"
      ],
      "metadata": {
        "id": "iW4Sf9seL4-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=filter_df, x=filter_df[0], y=filter_df[1], hue=filter_df[0], palette='bright', legend=False)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w4qeQtEsL6Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Find top 30 words of Not spam Messages"
      ],
      "metadata": {
        "id": "5izr_mPfMePU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"model\" height=\"70px\" src=\"https://raw.githubusercontent.com/isandrade-udea/LabIA/main/Captura%20desde%202024-05-24%2012-44-30.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "**Ejercicio**:\n",
        "Mostrar un histograma del top 30 de palabras mas frecuentes en mensajes ham\n"
      ],
      "metadata": {
        "id": "uaAoqLXYhPTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. Model Building**"
      ],
      "metadata": {
        "id": "0EWasPfkNkSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing CountVectorizer and TfidfVectorizer"
      ],
      "metadata": {
        "id": "-UtoFDQ_NpFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p><img alt=\"model\" height=\"340px\" src=\"https://raw.githubusercontent.com/isandrade-udea/datasets/main/Captura%20desde%202024-05-23%2016-01-56.png\" align=\"centering\" hspace=\"10px\" vspace=\"0px\"></p>\n"
      ],
      "metadata": {
        "id": "C2fouNJAS8Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "cv = CountVectorizer()\n",
        "tfid = TfidfVectorizer(max_features = 3000)"
      ],
      "metadata": {
        "id": "mIZ1bVzETfGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dependent and Independent Variable"
      ],
      "metadata": {
        "id": "6rmJUEZ2OftT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tfid.fit_transform(df['transformed_text']).toarray()\n",
        "y = df['target'].values"
      ],
      "metadata": {
        "id": "KJtKG3TmMyqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split into Train and Test Data"
      ],
      "metadata": {
        "id": "YhsW0--yTr5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test , y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state = 2)"
      ],
      "metadata": {
        "id": "E5I5ZpXcTmDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import the Models"
      ],
      "metadata": {
        "id": "YbntvpDBT4b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "exloa7oNTy8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize the Models"
      ],
      "metadata": {
        "id": "eEg8PE-_UJ9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SVC?"
      ],
      "metadata": {
        "id": "ZtYlg0eMc4aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC(kernel= \"sigmoid\", gamma  = 1.0)\n",
        "knc = KNeighborsClassifier()\n",
        "mnb = MultinomialNB()\n",
        "dtc = DecisionTreeClassifier(max_depth = 5)\n",
        "lrc = LogisticRegression(solver = 'liblinear', penalty = 'l1')\n",
        "rfc = RandomForestClassifier(n_estimators = 50, random_state = 2 )\n",
        "abc = AdaBoostClassifier(n_estimators = 50, random_state = 2)\n",
        "bc = BaggingClassifier(n_estimators = 50, random_state = 2)\n",
        "etc = ExtraTreesClassifier(n_estimators = 50, random_state = 2)\n",
        "gbdt = GradientBoostingClassifier(n_estimators = 50, random_state = 2)\n",
        "xgb  = XGBClassifier(n_estimators = 50, random_state = 2)"
      ],
      "metadata": {
        "id": "jxlWsrbPUCGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dictionary of the Models"
      ],
      "metadata": {
        "id": "HXKqj5z-UTph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = {\n",
        "    'SVC': svc,\n",
        "    'KNN': knc,\n",
        "    'NB': mnb,\n",
        "    'DT': dtc,\n",
        "    'LR': lrc,\n",
        "    'RF': rfc,\n",
        "    'Adaboost': abc,\n",
        "    'Bgc': bc,\n",
        "    'ETC': etc,\n",
        "    'GBDT': gbdt,\n",
        "    'xgb': xgb\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "RJcsM_dUUPyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the Models"
      ],
      "metadata": {
        "id": "3hVIRnN2UX68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "def train_classifier(clfs, X_train, y_train, X_test, y_test):\n",
        "    clfs.fit(X_train,y_train)\n",
        "    y_pred = clfs.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    return accuracy , precision"
      ],
      "metadata": {
        "id": "5b9dp2UmUVtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8. Evaluate the Models**"
      ],
      "metadata": {
        "id": "eU3dc5VfUnei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "for name , clfs in clfs.items():\n",
        "    current_accuracy, current_precision = train_classifier(clfs, X_train, y_train, X_test, y_test)\n",
        "    print()\n",
        "    print(\"For: \", name)\n",
        "    print(\"Accuracy: \", current_accuracy)\n",
        "    print(\"Precision: \", current_precision)\n",
        "\n",
        "    accuracy_scores.append(current_accuracy)\n",
        "    precision_scores.append(current_precision)"
      ],
      "metadata": {
        "id": "_4mrlpYlUjuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Software Version**"
      ],
      "metadata": {
        "id": "LqhsQ8LTeECA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install session_info"
      ],
      "metadata": {
        "id": "1n7cRScMbZWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import session_info\n",
        "session_info.show()"
      ],
      "metadata": {
        "id": "FHgvXEdNU6vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dE1WcJvpearm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11BzEtO647UNlj85dqi3XHXXzi8PMjgYO",
      "authorship_tag": "ABX9TyNxEnL3Ecmtyfx2HY6lderS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}